{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem  import  PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure that all NLTK packages are downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Choose a text to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''I dropped out of Reed College after the first 6 months, but then stayed around as a drop-in for another 18 months or so before I really quit. So why did I drop out? It started before I was born. My biological mother was a young, unwed college graduate student, and she decided to put me up for adoption.'''\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tokenization\n",
    "Tokenization is about creating tokens. Tokens are small portions of text. They can be words, more complex phrases, or even the whole sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Tokenizing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitted_dot = text.split('.')\n",
    "\n",
    "for index in range(len(text_splitted_dot)):\n",
    "    print(str(index+1)+'.',text_splitted_dot[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "print(type(sentences))\n",
    "for index in range(len(sentences)):\n",
    "    print(str(index+1)+'.',sentences[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Tokenizing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitted_space = text.split(' ')\n",
    "\n",
    "for index in range(20):\n",
    "    print(str(index+1)+'.',text_splitted_space[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tokenized = word_tokenize(text)\n",
    "    \n",
    "names = ['text_splitted_space'+' | ', 'words_tokenized']\n",
    "formatted_text = '{:>16}' * (len(names))\n",
    "print('\\n', formatted_text.format(*names),'\\n', '='*38)\n",
    "\n",
    "for index in range(20):\n",
    "    output = [text_splitted_space[index], words_tokenized[index]]\n",
    "    print(formatted_text.format(*output))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_punkt_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "words_punkt_tokenized = word_punkt_tokenizer.tokenize(text)\n",
    "\n",
    "names = ['text_splitted_space'+' | ', 'words_tokenized'+' | ', 'words_punkt_tokenized']\n",
    "formatted_text = '{:>18}' * (len(names))\n",
    "print('\\n', formatted_text.format(*names),'\\n', '='*61)\n",
    "\n",
    "for index in range(21):\n",
    "    output = [text_splitted_space[index], words_tokenized[index],words_punkt_tokenized[index]]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Text chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the input text into chunks, where each chunk contains N words\n",
    "def chunker(input_data, N):\n",
    "    input_words = input_data.split(' ')\n",
    "    output = []\n",
    "\n",
    "    current_chunk = []\n",
    "    count = 0\n",
    "    for word in input_words:\n",
    "        current_chunk.append(word)\n",
    "        count += 1\n",
    "        if count == N:\n",
    "            output.append(' '.join(current_chunk))\n",
    "            count, current_chunk = 0, []\n",
    "\n",
    "    output.append(' '.join(current_chunk))\n",
    "\n",
    "    return output\n",
    "\n",
    "# Define the number of words in each chunk\n",
    "chunk_size = 6\n",
    "\n",
    "chunks = chunker(text, chunk_size)\n",
    "print('\\nNumber of text chunks =', len(chunks), '\\n')\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print('Chunk', i+1, '==>', chunk) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "words_stemmed = [porter_stemmer.stem(word) for word in words_tokenized]\n",
    "\n",
    "stemmed_sentences =[]\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    stemmed_sentences.append(words)\n",
    "\n",
    "for index in range(len(stemmed_sentences)):\n",
    "    print(str(index+1)+'.',stemmed_sentences[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# Create a list of stemmer names for display\n",
    "names = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "formatted_text = '{:>16}' * (len(names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *names),'\\n', '='*70)\n",
    "\n",
    "# Stem each word and display the output\n",
    "for word in words_tokenized[:29]:\n",
    "    output = [word, porter_stemmer.stem(word), \n",
    "            lancaster.stem(word), snowball.stem(word)]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words_lemmatized = [lemmatizer.lemmatize(word) for word in words_tokenized]\n",
    "\n",
    "lemmatized_sentences =[]\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    lemmatized_sentences.append(words)\n",
    "    \n",
    "for index in range(len(lemmatized_sentences)):\n",
    "    print(str(index+1)+'.',lemmatized_sentences[index]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['snowball stemmer', 'lemmatizer noun', 'lemmatizer verb']\n",
    "formatted_text = '{:>20}' * (len(names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *names),'\\n', '='*88)\n",
    "\n",
    "for word in words_tokenized:\n",
    "    output = [word, \n",
    "              snowball.stem(word), \n",
    "              lemmatizer.lemmatize(word), \n",
    "              lemmatizer.lemmatize(word, pos='v')]\n",
    "    print(formatted_text.format(*output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

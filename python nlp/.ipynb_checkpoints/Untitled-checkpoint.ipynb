{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem  import  PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure that all NLTK packages are downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Choose a text to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I dropped out of Reed College after the first 6 months, but then stayed around as a drop-in for another 18 \n",
      "months or so before I really quit. So why did I drop out? It started before I was born. My biological mother was \n",
      "a young, unwed college graduate student, and she decided to put me up for adoption.\n"
     ]
    }
   ],
   "source": [
    "text = '''I dropped out of Reed College after the first 6 months, but then stayed around as a drop-in for another 18 \n",
    "months or so before I really quit. So why did I drop out? It started before I was born. My biological mother was \n",
    "a young, unwed college graduate student, and she decided to put me up for adoption.'''\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tokenization\n",
    "Tokenization is about creating tokens. Tokens are small portions of text. They can be words, more complex phrases, or even the whole sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Tokenizing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I dropped out of Reed College after the first 6 months, but then stayed around as a drop-in for another 18 \n",
      "months or so before I really quit\n",
      "2.  So why did I drop out? It started before I was born\n",
      "3.  My biological mother was \n",
      "a young, unwed college graduate student, and she decided to put me up for adoption\n",
      "4. \n"
     ]
    }
   ],
   "source": [
    "text_splitted_dot = text.split('.')\n",
    "\n",
    "for index in range(len(text_splitted_dot)):\n",
    "    print(str(index+1)+'.',text_splitted_dot[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I dropped out of Reed College after the first 6 months, but then stayed around as a drop-in for another 18 \n",
      "months or so before I really quit.\n",
      "2. So why did I drop out?\n",
      "3. It started before I was born.\n",
      "4. My biological mother was \n",
      "a young, unwed college graduate student, and she decided to put me up for adoption.\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "\n",
    "for index in range(len(sentences)):\n",
    "    print(str(index+1)+'.',sentences[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Tokenizing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I\n",
      "2. dropped\n",
      "3. out\n",
      "4. of\n",
      "5. Reed\n",
      "6. College\n",
      "7. after\n",
      "8. the\n",
      "9. first\n",
      "10. 6\n",
      "11. months,\n",
      "12. but\n",
      "13. then\n",
      "14. stayed\n",
      "15. around\n",
      "16. as\n",
      "17. a\n",
      "18. drop-in\n",
      "19. for\n",
      "20. another\n"
     ]
    }
   ],
   "source": [
    "text_splitted_space = text.split()\n",
    "\n",
    "for index in range(20):\n",
    "    print(str(index+1)+'.',text_splitted_space[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " text_splitted_space |  words_tokenized \n",
      " ======================================\n",
      "               I               I\n",
      "         dropped         dropped\n",
      "             out             out\n",
      "              of              of\n",
      "            Reed            Reed\n",
      "         College         College\n",
      "           after           after\n",
      "             the             the\n",
      "           first           first\n",
      "               6               6\n",
      "         months,          months\n",
      "             but               ,\n",
      "            then             but\n",
      "          stayed            then\n",
      "          around          stayed\n",
      "              as          around\n",
      "               a              as\n",
      "         drop-in               a\n",
      "             for         drop-in\n",
      "         another             for\n"
     ]
    }
   ],
   "source": [
    "words_tokenized = word_tokenize(text)\n",
    "    \n",
    "names = ['text_splitted_space'+' | ', 'words_tokenized']\n",
    "formatted_text = '{:>16}' * (len(names))\n",
    "print('\\n', formatted_text.format(*names),'\\n', '='*38)\n",
    "\n",
    "for index in range(20):\n",
    "    output = [text_splitted_space[index], words_tokenized[index]]\n",
    "    print(formatted_text.format(*output))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " text_splitted_space | words_tokenized | words_punkt_tokenized \n",
      " =============================================================\n",
      "                 I                 I                 I\n",
      "           dropped           dropped           dropped\n",
      "               out               out               out\n",
      "                of                of                of\n",
      "              Reed              Reed              Reed\n",
      "           College           College           College\n",
      "             after             after             after\n",
      "               the               the               the\n",
      "             first             first             first\n",
      "                 6                 6                 6\n",
      "           months,            months            months\n",
      "               but                 ,                 ,\n",
      "              then               but               but\n",
      "            stayed              then              then\n",
      "            around            stayed            stayed\n",
      "                as            around            around\n",
      "                 a                as                as\n",
      "           drop-in                 a                 a\n",
      "               for           drop-in              drop\n",
      "           another               for                 -\n",
      "                18           another                in\n"
     ]
    }
   ],
   "source": [
    "word_punkt_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "words_punkt_tokenized = word_punkt_tokenizer.tokenize(text)\n",
    "\n",
    "names = ['text_splitted_space'+' | ', 'words_tokenized'+' | ', 'words_punkt_tokenized']\n",
    "formatted_text = '{:>18}' * (len(names))\n",
    "print('\\n', formatted_text.format(*names),'\\n', '='*61)\n",
    "\n",
    "for index in range(21):\n",
    "    output = [text_splitted_space[index], words_tokenized[index],words_punkt_tokenized[index]]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Text chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of text chunks = 6 \n",
      "\n",
      "Chunk 1 ==> I dropped out of Reed College after the first 6\n",
      "Chunk 2 ==> months, but then stayed around as a drop-in for another\n",
      "Chunk 3 ==> 18 \n",
      "months or so before I really quit. So why\n",
      "Chunk 4 ==> did I drop out? It started before I was born.\n",
      "Chunk 5 ==> My biological mother was \n",
      "a young, unwed college graduate student,\n",
      "Chunk 6 ==> and she decided to put me up for adoption.\n"
     ]
    }
   ],
   "source": [
    "# Split the input text into chunks, where each chunk contains N words\n",
    "def chunker(input_data, N):\n",
    "    input_words = input_data.split(' ')\n",
    "    output = []\n",
    "\n",
    "    cur_chunk = []\n",
    "    count = 0\n",
    "    for word in input_words:\n",
    "        cur_chunk.append(word)\n",
    "        count += 1\n",
    "        if count == N:\n",
    "            output.append(' '.join(cur_chunk))\n",
    "            count, cur_chunk = 0, []\n",
    "\n",
    "    output.append(' '.join(cur_chunk))\n",
    "\n",
    "    return output\n",
    "\n",
    "# Define the number of words in each chunk\n",
    "chunk_size = 10\n",
    "\n",
    "chunks = chunker(text, chunk_size)\n",
    "print('\\nNumber of text chunks =', len(chunks), '\\n')\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print('Chunk', i+1, '==>', chunk) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ['I', 'drop', 'out', 'of', 'reed', 'colleg', 'after', 'the', 'first', '6', 'month', ',', 'but', 'then', 'stay', 'around', 'as', 'a', 'drop-in', 'for', 'anoth', '18', 'month', 'or', 'so', 'befor', 'I', 'realli', 'quit', '.']\n",
      "2. ['So', 'whi', 'did', 'I', 'drop', 'out', '?']\n",
      "3. ['It', 'start', 'befor', 'I', 'wa', 'born', '.']\n",
      "4. ['My', 'biolog', 'mother', 'wa', 'a', 'young', ',', 'unw', 'colleg', 'graduat', 'student', ',', 'and', 'she', 'decid', 'to', 'put', 'me', 'up', 'for', 'adopt', '.']\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "words_stemmed = [porter_stemmer.stem(word) for word in words_tokenized]\n",
    "\n",
    "stemmed_sentences =[]\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    stemmed_sentences.append(words)\n",
    "\n",
    "for index in range(len(stemmed_sentences)):\n",
    "    print(str(index+1)+'.',stemmed_sentences[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER       LANCASTER        SNOWBALL \n",
      " ======================================================================\n",
      "               I               I               i               i\n",
      "         dropped            drop            drop            drop\n",
      "             out             out             out             out\n",
      "              of              of              of              of\n",
      "            Reed            reed             ree            reed\n",
      "         College          colleg          colleg          colleg\n",
      "           after           after             aft           after\n",
      "             the             the             the             the\n",
      "           first           first           first           first\n",
      "               6               6               6               6\n",
      "          months           month           month           month\n",
      "               ,               ,               ,               ,\n",
      "             but             but             but             but\n",
      "            then            then            then            then\n",
      "          stayed            stay            stay            stay\n",
      "          around          around          around          around\n",
      "              as              as              as              as\n",
      "               a               a               a               a\n",
      "         drop-in         drop-in         drop-in         drop-in\n",
      "             for             for             for             for\n",
      "         another           anoth           anoth           anoth\n",
      "              18              18              18              18\n",
      "          months           month           month           month\n",
      "              or              or              or              or\n",
      "              so              so              so              so\n",
      "          before           befor             bef           befor\n",
      "               I               I               i               i\n",
      "          really          realli            real          realli\n",
      "            quit            quit            quit            quit\n"
     ]
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# Create a list of stemmer names for display\n",
    "names = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "formatted_text = '{:>16}' * (len(names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *names),'\\n', '='*70)\n",
    "\n",
    "# Stem each word and display the output\n",
    "for word in words_tokenized[:29]:\n",
    "    output = [word, porter_stemmer.stem(word), \n",
    "            lancaster.stem(word), snowball.stem(word)]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ['I', 'dropped', 'out', 'of', 'Reed', 'College', 'after', 'the', 'first', '6', 'month', ',', 'but', 'then', 'stayed', 'around', 'a', 'a', 'drop-in', 'for', 'another', '18', 'month', 'or', 'so', 'before', 'I', 'really', 'quit', '.']\n",
      "2. ['So', 'why', 'did', 'I', 'drop', 'out', '?']\n",
      "3. ['It', 'started', 'before', 'I', 'wa', 'born', '.']\n",
      "4. ['My', 'biological', 'mother', 'wa', 'a', 'young', ',', 'unwed', 'college', 'graduate', 'student', ',', 'and', 'she', 'decided', 'to', 'put', 'me', 'up', 'for', 'adoption', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words_lemmatized = [lemmatizer.lemmatize(word) for word in words_tokenized]\n",
    "\n",
    "lemmatized_sentences =[]\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    lemmatized_sentences.append(words)\n",
    "    \n",
    "for index in range(len(lemmatized_sentences)):\n",
    "    print(str(index+1)+'.',lemmatized_sentences[index]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           INPUT WORD    snowball stemmer     lemmatizer noun     lemmatizer verb \n",
      " ========================================================================================\n",
      "                   I                   i                   I                   I\n",
      "             dropped                drop             dropped                drop\n",
      "                 out                 out                 out                 out\n",
      "                  of                  of                  of                  of\n",
      "                Reed                reed                Reed                Reed\n",
      "             College              colleg             College             College\n",
      "               after               after               after               after\n",
      "                 the                 the                 the                 the\n",
      "               first               first               first               first\n",
      "                   6                   6                   6                   6\n",
      "              months               month               month              months\n",
      "                   ,                   ,                   ,                   ,\n",
      "                 but                 but                 but                 but\n",
      "                then                then                then                then\n",
      "              stayed                stay              stayed                stay\n",
      "              around              around              around              around\n",
      "                  as                  as                   a                  as\n",
      "                   a                   a                   a                   a\n",
      "             drop-in             drop-in             drop-in             drop-in\n",
      "                 for                 for                 for                 for\n",
      "             another               anoth             another             another\n",
      "                  18                  18                  18                  18\n",
      "              months               month               month              months\n",
      "                  or                  or                  or                  or\n",
      "                  so                  so                  so                  so\n",
      "              before               befor              before              before\n",
      "                   I                   i                   I                   I\n",
      "              really              realli              really              really\n",
      "                quit                quit                quit                quit\n",
      "                   .                   .                   .                   .\n",
      "                  So                  so                  So                  So\n",
      "                 why                 whi                 why                 why\n",
      "                 did                 did                 did                  do\n",
      "                   I                   i                   I                   I\n",
      "                drop                drop                drop                drop\n",
      "                 out                 out                 out                 out\n",
      "                   ?                   ?                   ?                   ?\n",
      "                  It                  it                  It                  It\n",
      "             started               start             started               start\n",
      "              before               befor              before              before\n",
      "                   I                   i                   I                   I\n",
      "                 was                 was                  wa                  be\n",
      "                born                born                born                bear\n",
      "                   .                   .                   .                   .\n",
      "                  My                  my                  My                  My\n",
      "          biological              biolog          biological          biological\n",
      "              mother              mother              mother              mother\n",
      "                 was                 was                  wa                  be\n",
      "                   a                   a                   a                   a\n",
      "               young               young               young               young\n",
      "                   ,                   ,                   ,                   ,\n",
      "               unwed                 unw               unwed               unwed\n",
      "             college              colleg             college             college\n",
      "            graduate             graduat            graduate            graduate\n",
      "             student             student             student             student\n",
      "                   ,                   ,                   ,                   ,\n",
      "                 and                 and                 and                 and\n",
      "                 she                 she                 she                 she\n",
      "             decided               decid             decided              decide\n",
      "                  to                  to                  to                  to\n",
      "                 put                 put                 put                 put\n",
      "                  me                  me                  me                  me\n",
      "                  up                  up                  up                  up\n",
      "                 for                 for                 for                 for\n",
      "            adoption               adopt            adoption            adoption\n",
      "                   .                   .                   .                   .\n"
     ]
    }
   ],
   "source": [
    "names = ['snowball stemmer', 'lemmatizer noun', 'lemmatizer verb']\n",
    "formatted_text = '{:>20}' * (len(names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *names),'\\n', '='*88)\n",
    "\n",
    "for word in words_tokenized:\n",
    "    output = [word, \n",
    "              snowball.stem(word), \n",
    "              lemmatizer.lemmatize(word), \n",
    "              lemmatizer.lemmatize(word, pos='v')]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['18', 'adoption', 'after', 'and', 'another', 'around', 'as', 'before', 'biological', 'born', 'but', 'college', 'decided', 'did', 'drop', 'dropped', 'first', 'for', 'graduate', 'in', 'it', 'me', 'months', 'mother', 'my', 'of', 'or', 'out', 'put', 'quit', 'really', 'reed', 'she', 'so', 'started', 'stayed', 'student', 'the', 'then', 'to', 'unwed', 'up', 'was', 'why', 'young']\n",
      "[[1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 2 0 0 1 1 1 0 1 1 1 0 1 0 1\n",
      "  0 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 1 0 0]\n",
      " [0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0\n",
      "  1 0 0 1 1 1 1 0 1]]\n",
      "Word \" adoption \" appears 1 times in sentence: My biological mother was \n",
      "a young, unwed college graduate student, and she decided to put me up for adoption.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "print(vocabulary)\n",
    "print(X.toarray())\n",
    "\n",
    "word_nr = 1\n",
    "sentence_nr = 3\n",
    "print('Word \\\"',vocabulary[word_nr],'\\\" appears', X.toarray()[sentence_nr,word_nr],'times in sentence:', sentences[sentence_nr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['18 months', 'after the', 'and she', 'another 18', 'around as', 'as drop', 'before really', 'before was', 'biological mother', 'but then', 'college after', 'college graduate', 'decided to', 'did drop', 'drop in', 'drop out', 'dropped out', 'first months', 'for adoption', 'for another', 'graduate student', 'in for', 'it started', 'me up', 'months but', 'months or', 'mother was', 'my biological', 'of reed', 'or so', 'out of', 'put me', 'really quit', 'reed college', 'she decided', 'so before', 'so why', 'started before', 'stayed around', 'student and', 'the first', 'then stayed', 'to put', 'unwed college', 'up for', 'was born', 'was young', 'why did', 'young unwed']\n",
      "[[1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1\n",
      "  0 0 1 0 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0\n",
      "  0 0 0 1 0 0 1 1 1 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "\n",
    "X2 = vectorizer2.fit_transform(sentences)\n",
    "\n",
    "print(vectorizer2.get_feature_names())\n",
    "\n",
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

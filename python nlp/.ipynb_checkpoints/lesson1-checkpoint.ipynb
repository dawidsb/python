{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem  import  PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure that all NLTK packages are downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Choose a text to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I dropped out of Reed College after the first 6 months, but then stayed around as a drop-in for another 18 months or so before I really quit. So why did I drop out? It started before I was born. My biological mother was a young, unwed college graduate student, and she decided to put me up for adoption.\n"
     ]
    }
   ],
   "source": [
    "text = '''I dropped out of Reed College after the first 6 months, but then stayed around as a drop-in for another 18 months or so before I really quit. So why did I drop out? It started before I was born. My biological mother was a young, unwed college graduate student, and she decided to put me up for adoption.'''\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tokenization\n",
    "Tokenization is about creating tokens. Tokens are small portions of text. They can be words, more complex phrases, or even the whole sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Tokenizing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I dropped out of Reed College after the first 6 months, but then stayed around as a drop-in for another 18 months or so before I really quit\n",
      "2.  So why did I drop out? It started before I was born\n",
      "3.  My biological mother was a young, unwed college graduate student, and she decided to put me up for adoption\n",
      "4. \n"
     ]
    }
   ],
   "source": [
    "text_splitted_dot = text.split('.')\n",
    "\n",
    "for index in range(len(text_splitted_dot)):\n",
    "    print(str(index+1)+'.',text_splitted_dot[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1. I dropped out of Reed College after the first 6 months, but then stayed around as a drop-in for another 18 months or so before I really quit.\n",
      "2. So why did I drop out?\n",
      "3. It started before I was born.\n",
      "4. My biological mother was a young, unwed college graduate student, and she decided to put me up for adoption.\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "print(type(sentences))\n",
    "for index in range(len(sentences)):\n",
    "    print(str(index+1)+'.',sentences[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Tokenizing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I\n",
      "2. dropped\n",
      "3. out\n",
      "4. of\n",
      "5. Reed\n",
      "6. College\n",
      "7. after\n",
      "8. the\n",
      "9. first\n",
      "10. 6\n",
      "11. months,\n",
      "12. but\n",
      "13. then\n",
      "14. stayed\n",
      "15. around\n",
      "16. as\n",
      "17. a\n",
      "18. drop-in\n",
      "19. for\n",
      "20. another\n"
     ]
    }
   ],
   "source": [
    "text_splitted_space = text.split(' ')\n",
    "\n",
    "for index in range(20):\n",
    "    print(str(index+1)+'.',text_splitted_space[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " text_splitted_space |  words_tokenized \n",
      " ======================================\n",
      "               I               I\n",
      "         dropped         dropped\n",
      "             out             out\n",
      "              of              of\n",
      "            Reed            Reed\n",
      "         College         College\n",
      "           after           after\n",
      "             the             the\n",
      "           first           first\n",
      "               6               6\n",
      "         months,          months\n",
      "             but               ,\n",
      "            then             but\n",
      "          stayed            then\n",
      "          around          stayed\n",
      "              as          around\n",
      "               a              as\n",
      "         drop-in               a\n",
      "             for         drop-in\n",
      "         another             for\n"
     ]
    }
   ],
   "source": [
    "words_tokenized = word_tokenize(text)\n",
    "    \n",
    "names = ['text_splitted_space'+' | ', 'words_tokenized']\n",
    "formatted_text = '{:>16}' * (len(names))\n",
    "print('\\n', formatted_text.format(*names),'\\n', '='*38)\n",
    "\n",
    "for index in range(20):\n",
    "    output = [text_splitted_space[index], words_tokenized[index]]\n",
    "    print(formatted_text.format(*output))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " text_splitted_space | words_tokenized | words_punkt_tokenized \n",
      " =============================================================\n",
      "                 I                 I                 I\n",
      "           dropped           dropped           dropped\n",
      "               out               out               out\n",
      "                of                of                of\n",
      "              Reed              Reed              Reed\n",
      "           College           College           College\n",
      "             after             after             after\n",
      "               the               the               the\n",
      "             first             first             first\n",
      "                 6                 6                 6\n",
      "           months,            months            months\n",
      "               but                 ,                 ,\n",
      "              then               but               but\n",
      "            stayed              then              then\n",
      "            around            stayed            stayed\n",
      "                as            around            around\n",
      "                 a                as                as\n",
      "           drop-in                 a                 a\n",
      "               for           drop-in              drop\n",
      "           another               for                 -\n",
      "                18           another                in\n"
     ]
    }
   ],
   "source": [
    "word_punkt_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "words_punkt_tokenized = word_punkt_tokenizer.tokenize(text)\n",
    "\n",
    "names = ['text_splitted_space'+' | ', 'words_tokenized'+' | ', 'words_punkt_tokenized']\n",
    "formatted_text = '{:>18}' * (len(names))\n",
    "print('\\n', formatted_text.format(*names),'\\n', '='*61)\n",
    "\n",
    "for index in range(21):\n",
    "    output = [text_splitted_space[index], words_tokenized[index],words_punkt_tokenized[index]]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Text chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of text chunks = 10 \n",
      "\n",
      "Chunk 1 ==> I dropped out of Reed College\n",
      "Chunk 2 ==> after the first 6 months, but\n",
      "Chunk 3 ==> then stayed around as a drop-in\n",
      "Chunk 4 ==> for another 18 months or so\n",
      "Chunk 5 ==> before I really quit. So why\n",
      "Chunk 6 ==> did I drop out? It started\n",
      "Chunk 7 ==> before I was born. My biological\n",
      "Chunk 8 ==> mother was a young, unwed college\n",
      "Chunk 9 ==> graduate student, and she decided to\n",
      "Chunk 10 ==> put me up for adoption.\n"
     ]
    }
   ],
   "source": [
    "# Split the input text into chunks, where each chunk contains N words\n",
    "def chunker(input_data, N):\n",
    "    input_words = input_data.split(' ')\n",
    "    output = []\n",
    "\n",
    "    current_chunk = []\n",
    "    count = 0\n",
    "    for word in input_words:\n",
    "        current_chunk.append(word)\n",
    "        count += 1\n",
    "        if count == N:\n",
    "            output.append(' '.join(current_chunk))\n",
    "            count, current_chunk = 0, []\n",
    "\n",
    "    output.append(' '.join(current_chunk))\n",
    "\n",
    "    return output\n",
    "\n",
    "# Define the number of words in each chunk\n",
    "chunk_size = 6\n",
    "\n",
    "chunks = chunker(text, chunk_size)\n",
    "print('\\nNumber of text chunks =', len(chunks), '\\n')\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print('Chunk', i+1, '==>', chunk) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ['I', 'drop', 'out', 'of', 'reed', 'colleg', 'after', 'the', 'first', '6', 'month', ',', 'but', 'then', 'stay', 'around', 'as', 'a', 'drop-in', 'for', 'anoth', '18', 'month', 'or', 'so', 'befor', 'I', 'realli', 'quit', '.']\n",
      "2. ['So', 'whi', 'did', 'I', 'drop', 'out', '?']\n",
      "3. ['It', 'start', 'befor', 'I', 'wa', 'born', '.']\n",
      "4. ['My', 'biolog', 'mother', 'wa', 'a', 'young', ',', 'unw', 'colleg', 'graduat', 'student', ',', 'and', 'she', 'decid', 'to', 'put', 'me', 'up', 'for', 'adopt', '.']\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "words_stemmed = [porter_stemmer.stem(word) for word in words_tokenized]\n",
    "\n",
    "stemmed_sentences =[]\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    stemmed_sentences.append(words)\n",
    "\n",
    "for index in range(len(stemmed_sentences)):\n",
    "    print(str(index+1)+'.',stemmed_sentences[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER       LANCASTER        SNOWBALL \n",
      " ======================================================================\n",
      "               I               I               i               i\n",
      "         dropped            drop            drop            drop\n",
      "             out             out             out             out\n",
      "              of              of              of              of\n",
      "            Reed            reed             ree            reed\n",
      "         College          colleg          colleg          colleg\n",
      "           after           after             aft           after\n",
      "             the             the             the             the\n",
      "           first           first           first           first\n",
      "               6               6               6               6\n",
      "          months           month           month           month\n",
      "               ,               ,               ,               ,\n",
      "             but             but             but             but\n",
      "            then            then            then            then\n",
      "          stayed            stay            stay            stay\n",
      "          around          around          around          around\n",
      "              as              as              as              as\n",
      "               a               a               a               a\n",
      "         drop-in         drop-in         drop-in         drop-in\n",
      "             for             for             for             for\n",
      "         another           anoth           anoth           anoth\n",
      "              18              18              18              18\n",
      "          months           month           month           month\n",
      "              or              or              or              or\n",
      "              so              so              so              so\n",
      "          before           befor             bef           befor\n",
      "               I               I               i               i\n",
      "          really          realli            real          realli\n",
      "            quit            quit            quit            quit\n"
     ]
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# Create a list of stemmer names for display\n",
    "names = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "formatted_text = '{:>16}' * (len(names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *names),'\\n', '='*70)\n",
    "\n",
    "# Stem each word and display the output\n",
    "for word in words_tokenized[:29]:\n",
    "    output = [word, porter_stemmer.stem(word), \n",
    "            lancaster.stem(word), snowball.stem(word)]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ['I', 'dropped', 'out', 'of', 'Reed', 'College', 'after', 'the', 'first', '6', 'month', ',', 'but', 'then', 'stayed', 'around', 'a', 'a', 'drop-in', 'for', 'another', '18', 'month', 'or', 'so', 'before', 'I', 'really', 'quit', '.']\n",
      "2. ['So', 'why', 'did', 'I', 'drop', 'out', '?']\n",
      "3. ['It', 'started', 'before', 'I', 'wa', 'born', '.']\n",
      "4. ['My', 'biological', 'mother', 'wa', 'a', 'young', ',', 'unwed', 'college', 'graduate', 'student', ',', 'and', 'she', 'decided', 'to', 'put', 'me', 'up', 'for', 'adoption', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words_lemmatized = [lemmatizer.lemmatize(word) for word in words_tokenized]\n",
    "\n",
    "lemmatized_sentences =[]\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    lemmatized_sentences.append(words)\n",
    "    \n",
    "for index in range(len(lemmatized_sentences)):\n",
    "    print(str(index+1)+'.',lemmatized_sentences[index]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           INPUT WORD    snowball stemmer     lemmatizer noun     lemmatizer verb \n",
      " ========================================================================================\n",
      "                   I                   i                   I                   I\n",
      "             dropped                drop             dropped                drop\n",
      "                 out                 out                 out                 out\n",
      "                  of                  of                  of                  of\n",
      "                Reed                reed                Reed                Reed\n",
      "             College              colleg             College             College\n",
      "               after               after               after               after\n",
      "                 the                 the                 the                 the\n",
      "               first               first               first               first\n",
      "                   6                   6                   6                   6\n",
      "              months               month               month              months\n",
      "                   ,                   ,                   ,                   ,\n",
      "                 but                 but                 but                 but\n",
      "                then                then                then                then\n",
      "              stayed                stay              stayed                stay\n",
      "              around              around              around              around\n",
      "                  as                  as                   a                  as\n",
      "                   a                   a                   a                   a\n",
      "             drop-in             drop-in             drop-in             drop-in\n",
      "                 for                 for                 for                 for\n",
      "             another               anoth             another             another\n",
      "                  18                  18                  18                  18\n",
      "              months               month               month              months\n",
      "                  or                  or                  or                  or\n",
      "                  so                  so                  so                  so\n",
      "              before               befor              before              before\n",
      "                   I                   i                   I                   I\n",
      "              really              realli              really              really\n",
      "                quit                quit                quit                quit\n",
      "                   .                   .                   .                   .\n",
      "                  So                  so                  So                  So\n",
      "                 why                 whi                 why                 why\n",
      "                 did                 did                 did                  do\n",
      "                   I                   i                   I                   I\n",
      "                drop                drop                drop                drop\n",
      "                 out                 out                 out                 out\n",
      "                   ?                   ?                   ?                   ?\n",
      "                  It                  it                  It                  It\n",
      "             started               start             started               start\n",
      "              before               befor              before              before\n",
      "                   I                   i                   I                   I\n",
      "                 was                 was                  wa                  be\n",
      "                born                born                born                bear\n",
      "                   .                   .                   .                   .\n",
      "                  My                  my                  My                  My\n",
      "          biological              biolog          biological          biological\n",
      "              mother              mother              mother              mother\n",
      "                 was                 was                  wa                  be\n",
      "                   a                   a                   a                   a\n",
      "               young               young               young               young\n",
      "                   ,                   ,                   ,                   ,\n",
      "               unwed                 unw               unwed               unwed\n",
      "             college              colleg             college             college\n",
      "            graduate             graduat            graduate            graduate\n",
      "             student             student             student             student\n",
      "                   ,                   ,                   ,                   ,\n",
      "                 and                 and                 and                 and\n",
      "                 she                 she                 she                 she\n",
      "             decided               decid             decided              decide\n",
      "                  to                  to                  to                  to\n",
      "                 put                 put                 put                 put\n",
      "                  me                  me                  me                  me\n",
      "                  up                  up                  up                  up\n",
      "                 for                 for                 for                 for\n",
      "            adoption               adopt            adoption            adoption\n",
      "                   .                   .                   .                   .\n"
     ]
    }
   ],
   "source": [
    "names = ['snowball stemmer', 'lemmatizer noun', 'lemmatizer verb']\n",
    "formatted_text = '{:>20}' * (len(names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *names),'\\n', '='*88)\n",
    "\n",
    "for word in words_tokenized:\n",
    "    output = [word, \n",
    "              snowball.stem(word), \n",
    "              lemmatizer.lemmatize(word), \n",
    "              lemmatizer.lemmatize(word, pos='v')]\n",
    "    print(formatted_text.format(*output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
